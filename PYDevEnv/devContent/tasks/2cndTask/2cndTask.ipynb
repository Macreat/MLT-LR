{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector projections "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## first task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical description and problem Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given two vectors $$ \\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^2 $$,  \n",
        "the **projection of** $$ \\mathbf{a} $$ **onto** $$ \\mathbf{b} $$ represents the component of $$ \\mathbf{a} $$ that lies along the direction of $$ \\mathbf{b} $$.  \n",
        "Geometrically, it is the *shadow* of $$ \\mathbf{a} $$ on the line defined by $$ \\mathbf{b} $$\n",
        "\n",
        "We compute this projection in two ways:\n",
        "\n",
        "1. **Analytical (closed form)** — from vector algebra.  \n",
        "2. **Gradient Descent (iterative)** — from minimizing a least-squares cost.\n",
        "\n",
        "Lets define it. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Analytical projection (Closed form)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "####  Definition via Dot Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The projection of $$ \\mathbf{a} $$ onto $$ \\mathbf{b} $$ is given by:\n",
        "\n",
        "$$\n",
        "\\operatorname{proj}_{\\mathbf{b}}(\\mathbf{a})\n",
        "= \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\mathbf{b}\\cdot\\mathbf{b}} \\, \\mathbf{b}, \n",
        "\\qquad \\text{for } \\mathbf{b}\\neq \\mathbf{0}.\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $$ \\mathbf{a}\\cdot\\mathbf{b} $$ → alignment between vectors.  \n",
        "- $$ \\mathbf{b}\\cdot\\mathbf{b} = \\|\\mathbf{b}\\|^2 $$ → squared magnitude of $$ \\mathbf{b} $$.  \n",
        "- Scalar coefficient:\n",
        "\n",
        "$$\n",
        "c^\\star = \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\mathbf{b}\\cdot\\mathbf{b}}\n",
        "$$\n",
        "\n",
        "Hence:\n",
        "\n",
        "$$\n",
        "\\operatorname{proj}_{\\mathbf{b}}(\\mathbf{a}) = c^\\star \\mathbf{b}\n",
        "$$\n",
        "\n",
        " If $$ \\mathbf{b} = \\mathbf{0} $$, the projection is undefined (or zero by convention).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Least-Squares Derivation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can derive the same result by minimizing:\n",
        "\n",
        "$$\n",
        "\\min_{c\\in\\mathbb{R}} \\; J(c) = \\|\\mathbf{a} - c\\mathbf{b}\\|^2\n",
        "$$\n",
        "\n",
        "Expanding the cost:\n",
        "\n",
        "$$\n",
        "J(c) = (\\mathbf{a}-c\\mathbf{b})^\\top(\\mathbf{a}-c\\mathbf{b})\n",
        "     = \\mathbf{a}^\\top\\mathbf{a} - 2c(\\mathbf{a}^\\top\\mathbf{b}) + c^2(\\mathbf{b}^\\top\\mathbf{b})\n",
        "$$\n",
        "\n",
        "Setting derivative to zero:\n",
        "\n",
        "$$\n",
        "\\frac{dJ}{dc} = -2(\\mathbf{a}\\cdot\\mathbf{b}) + 2c(\\mathbf{b}\\cdot\\mathbf{b}) = 0\n",
        "\\Rightarrow\n",
        "c^\\star = \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\mathbf{b}\\cdot\\mathbf{b}}\n",
        "$$\n",
        "\n",
        "Got same solution as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Orthogonal Decomposition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let:\n",
        "\n",
        "$$\n",
        "\\mathbf{p} = \\operatorname{proj}_{\\mathbf{b}}(\\mathbf{a}) = c^\\star \\mathbf{b}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{r} = \\mathbf{a} - \\mathbf{p}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = \\mathbf{p} + \\mathbf{r}, \\quad \\mathbf{r}\\cdot\\mathbf{b} = 0\n",
        "$$\n",
        "\n",
        "That is, the residual $$ \\mathbf{r} $$ is **orthogonal** to $$ \\mathbf{b} $$.\n",
        "\n",
        "And by the Pythagorean theorem:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{a}\\|^2 = \\|\\mathbf{p}\\|^2 + \\|\\mathbf{r}\\|^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Angle Interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If $$ \\theta $$ is the angle between $$ \\mathbf{a} $$ and $$ \\mathbf{b} $$:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}\\cdot\\mathbf{b} = \\|\\mathbf{a}\\|\\,\\|\\mathbf{b}\\|\\,\\cos\\theta\n",
        "\\Rightarrow\n",
        "c^\\star = \\frac{\\|\\mathbf{a}\\|\\cos\\theta}{\\|\\mathbf{b}\\|}\n",
        "$$\n",
        "\n",
        "Thus, the **length of the projection** is $$ \\|\\mathbf{a}\\|\\cos\\theta $$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### proceed with the gradiente descent algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Gradient descent formulation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We minimize the same cost iteratively:\n",
        "\n",
        "$$\n",
        "J(c) = \\|\\mathbf{a} - c\\mathbf{b}\\|^2 \n",
        "= \\mathbf{a}^\\top\\mathbf{a} - 2c(\\mathbf{a}^\\top\\mathbf{b}) + c^2(\\mathbf{b}^\\top\\mathbf{b})\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gradient and update rule\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Derivative:\n",
        "\n",
        "$$\n",
        "\\frac{dJ}{dc} = -2(\\mathbf{a}\\cdot\\mathbf{b}) + 2c(\\mathbf{b}\\cdot\\mathbf{b})\n",
        "$$\n",
        "\n",
        "Gradient descent update:\n",
        "\n",
        "$$\n",
        "c_{k+1} = c_k - \\eta \\frac{dJ}{dc}\n",
        "= c_k - \\eta[-2(\\mathbf{a}\\cdot\\mathbf{b}) + 2c_k(\\mathbf{b}\\cdot\\mathbf{b})]\n",
        "$$\n",
        "\n",
        "Simplified:\n",
        "\n",
        "$$\n",
        "c_{k+1} = (1 - 2\\eta\\|\\mathbf{b}\\|^2)c_k + 2\\eta(\\mathbf{a}\\cdot\\mathbf{b})\n",
        "$$\n",
        "\n",
        "At convergence:\n",
        "\n",
        "$$\n",
        "c^\\star = \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\mathbf{b}\\cdot\\mathbf{b}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convergence condition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convergence requires:\n",
        "\n",
        "$$\n",
        "0 < \\eta < \\frac{1}{\\|\\mathbf{b}\\|^2}\n",
        "$$\n",
        "\n",
        "A safer bound (from Lipschitz constant $$ L=2\\|\\mathbf{b}\\|^2 $$):\n",
        "\n",
        "$$\n",
        "0 < \\eta < \\frac{1}{2\\|\\mathbf{b}\\|^2}\n",
        "$$\n",
        "\n",
        "Convergence rate:\n",
        "\n",
        "$$\n",
        "|c_k - c^\\star| \\le \\rho^k |c_0 - c^\\star|, \\quad \\rho = |1 - 2\\eta\\|\\mathbf{b}\\|^2| < 1\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Practical Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Large $$ \\|\\mathbf{b}\\| $$ → use smaller $$ \\eta $$.  \n",
        "- Stop when $$ |c_{k+1}-c_k| < \\varepsilon $$ or cost stops decreasing.  \n",
        "- If $$ \\mathbf{b}=0 $$ → gradient vanishes → undefined projection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### code implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analytical vs gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "| Aspect | Analytical | Gradient Descent |\n",
        "|--------|-------------|------------------|\n",
        "| Method | Closed-form algebraic | Iterative numerical |\n",
        "| Formula | $$ c^\\star = \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\mathbf{b}\\cdot\\mathbf{b}} $$ | $$ c_{k+1} = c_k - \\eta\\,\\frac{dJ}{dc} $$ |\n",
        "| Speed | Instant | Depends on $$ \\eta $$ |\n",
        "| Accuracy | Exact | Approximate (convergent) |\n",
        "| Insight | Geometric | Optimization-based |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " **Summary:**  \n",
        "This exercise connects **geometry**, **linear algebra**, and **optimization**,  \n",
        "showing that both analytical and iterative methods describe the same projection in different mathematical frameworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
